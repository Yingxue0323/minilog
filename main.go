package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/pierrec/lz4/v4"
)

type LogEntry struct {
	Timestamp string   `json:"timestamp"`
	Level     string   `json:"level"`
	Server    string   `json:"server"`
	Message   string   `json:"message"`
	Metrics   *Metrics `json:"metrics,omitempty"` // å¯é€‰çš„ç›‘æ§æŒ‡æ ‡
}

// æ—¥å¿—å­˜å‚¨å¼•æ“ï¼ˆæ ¸å¿ƒï¼‰
type LogStorage struct {
	// å†…å­˜ç¼“å†²åŒºï¼ˆæœ€æ–°çš„æ—¥å¿—ï¼Œæœªå‹ç¼©ï¼‰
	memoryBuffer []LogEntry
	bufferMu     sync.RWMutex
	
	// é…ç½®å‚æ•°
	maxBufferSize   int           // æœ€å¤§ç¼“å†²æ¡æ•°
	maxBufferMemory int64         // æœ€å¤§ç¼“å†²å†…å­˜ï¼ˆå­—èŠ‚ï¼‰
	flushInterval   time.Duration // åˆ·ç›˜é—´éš”
	dataDir         string
	
	// ç»Ÿè®¡ä¿¡æ¯
	stats struct {
		TotalReceived   int64
		TotalCompressed int64
		CompressionRatio float64
	}
}

func NewLogStorage(dataDir string) *LogStorage {
	os.MkdirAll(dataDir, 0755)
	
	storage := &LogStorage{
		memoryBuffer:    make([]LogEntry, 0, 1000),
		maxBufferSize:   1000,                 // æ”’å¤Ÿ1000æ¡å°±å‹ç¼©
		maxBufferMemory: 10 * 1024 * 1024,     // æˆ–è€…è¶…è¿‡10MBå°±å‹ç¼©
		flushInterval:   60 * time.Second,     // æˆ–è€…è¶…è¿‡60ç§’å°±å‹ç¼©
		dataDir:         dataDir,
	}
	
	// å¯åŠ¨åå°å®šæ—¶å‹ç¼©ä»»åŠ¡
	go storage.backgroundFlusher()
	
	return storage
}

// æ¥æ”¶æ—¥å¿—ï¼ˆå®æ—¶å†™å…¥å†…å­˜ï¼‰
func (s *LogStorage) Append(log LogEntry) {
	s.bufferMu.Lock()
	defer s.bufferMu.Unlock()
	
	// æ·»åŠ åˆ°å†…å­˜ç¼“å†²
	s.memoryBuffer = append(s.memoryBuffer, log)
	s.stats.TotalReceived++
	
	// æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³å‹ç¼©ï¼ˆæ¡ä»¶è§¦å‘ï¼‰
	if len(s.memoryBuffer) >= s.maxBufferSize {
		go s.flushToDisk() // å¼‚æ­¥å‹ç¼©ï¼Œä¸é˜»å¡æ¥æ”¶
	}
}

// åå°å®šæ—¶ä»»åŠ¡ï¼ˆå®šæ—¶å‹ç¼©ï¼‰
func (s *LogStorage) backgroundFlusher() {
	ticker := time.NewTicker(s.flushInterval)
	for range ticker.C {
		s.flushToDisk()
	}
}

// å‹ç¼©å¹¶å†™å…¥ç£ç›˜
func (s *LogStorage) flushToDisk() {
	s.bufferMu.Lock()
	
	// å¦‚æœç¼“å†²åŒºä¸ºç©ºï¼Œç›´æ¥è¿”å›
	if len(s.memoryBuffer) == 0 {
		s.bufferMu.Unlock()
		return
	}
	
	// å–å‡ºç¼“å†²åŒºæ•°æ®ï¼ˆå¿«é€Ÿé‡Šæ”¾é”ï¼‰
	logsToCompress := make([]LogEntry, len(s.memoryBuffer))
	copy(logsToCompress, s.memoryBuffer)
	s.memoryBuffer = s.memoryBuffer[:0] // æ¸…ç©ºç¼“å†²åŒº
	
	s.bufferMu.Unlock()
	
	// ä¸‹é¢çš„æ“ä½œä¸æŒæœ‰é”ï¼Œä¸å½±å“æ–°æ—¥å¿—å†™å…¥
	
	// 1. åºåˆ—åŒ–ä¸ºæ–‡æœ¬
	var plainText bytes.Buffer
	for _, log := range logsToCompress {
		line := fmt.Sprintf("[%s] [%s] [%s] %s\n",
			log.Timestamp, log.Level, log.Server, log.Message)
		plainText.WriteString(line)
	}
	
	// 2. LZ4å‹ç¼©
	var compressed bytes.Buffer
	writer := lz4.NewWriter(&compressed)
	writer.Write(plainText.Bytes())
	writer.Close()
	
	// 3. å†™å…¥æ–‡ä»¶ï¼ˆæŒ‰å°æ—¶åˆ†ç‰‡ï¼‰
	hour := time.Now().Format("2006-01-02-15")
	filename := fmt.Sprintf("%s/logs-%s.lz4", s.dataDir, hour)
	
	f, _ := os.OpenFile(filename, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	
	// å†™å…¥åˆ†éš”ç¬¦ï¼ˆæ–¹ä¾¿åç»­åˆ†å—è¯»å–ï¼‰
	separator := fmt.Sprintf("===CHUNK_%d===\n", time.Now().Unix())
	f.WriteString(separator)
	f.Write(compressed.Bytes())
	f.Close()
	
	// 4. æ›´æ–°ç»Ÿè®¡
	s.stats.TotalCompressed += int64(len(logsToCompress))
	originalSize := plainText.Len()
	compressedSize := compressed.Len()
	ratio := float64(originalSize) / float64(compressedSize)
	s.stats.CompressionRatio = ratio
	
	fmt.Printf("ğŸ’¾ [Compressed] %d logs | %d B â†’ %d B | Ratio %.1f:1 | File: %s\n",
		len(logsToCompress), originalSize, compressedSize, ratio, filename)
}

// æŸ¥è¯¢æ—¥å¿—ï¼ˆå†…å­˜ + ç£ç›˜ï¼‰æ”¯æŒå¤šç»´åº¦ç­›é€‰
func (s *LogStorage) Query(keyword, server, level string, limit int) []LogEntry {
	results := make([]LogEntry, 0)
	keywordLower := strings.ToLower(keyword)
	serverLower := strings.ToLower(server)
	levelLower := strings.ToLower(level)
	
	// 1. å…ˆæŸ¥å†…å­˜ï¼ˆæœ€æ–°çš„æœªå‹ç¼©æ•°æ®ï¼‰
	s.bufferMu.RLock()
	for i := len(s.memoryBuffer) - 1; i >= 0 && len(results) < limit; i-- {
		log := s.memoryBuffer[i]
		if s.matchLogWithFilters(log, keywordLower, serverLower, levelLower) {
			results = append(results, log)
		}
	}
	s.bufferMu.RUnlock()
	
	// å¦‚æœå†…å­˜ä¸­å·²ç»å¤Ÿäº†ï¼Œç›´æ¥è¿”å›
	if len(results) >= limit {
		return results
	}
	
	// 2. å†æŸ¥ç£ç›˜ï¼ˆå‹ç¼©çš„å†å²æ•°æ®ï¼‰
	diskResults := s.queryDisk(keywordLower, serverLower, levelLower, limit-len(results))
	results = append(results, diskResults...)
	
	return results
}

// å¤šç»´åº¦åŒ¹é…ï¼ˆæ”¯æŒå…³é”®å­—ã€æœåŠ¡å™¨ã€çº§åˆ«ç­›é€‰ï¼‰
func (s *LogStorage) matchLogWithFilters(log LogEntry, keyword, server, level string) bool {
	// å…³é”®å­—åŒ¹é…
	if keyword != "" {
		matchKeyword := strings.Contains(strings.ToLower(log.Message), keyword) ||
			strings.Contains(strings.ToLower(log.Level), keyword) ||
			strings.Contains(strings.ToLower(log.Server), keyword)
		if !matchKeyword {
			return false
		}
	}
	
	// æœåŠ¡å™¨åŒ¹é…
	if server != "" && strings.ToLower(log.Server) != server {
		return false
	}
	
	// çº§åˆ«åŒ¹é…
	if level != "" && strings.ToLower(log.Level) != level {
		return false
	}
	
	return true
}

func (s *LogStorage) queryDisk(keyword, server, level string, limit int) []LogEntry {
	results := make([]LogEntry, 0)
	
	// è¯»å–å½“å‰å°æ—¶çš„å‹ç¼©æ–‡ä»¶
	hour := time.Now().Format("2006-01-02-15")
	filename := fmt.Sprintf("%s/logs-%s.lz4", s.dataDir, hour)
	
	data, err := os.ReadFile(filename)
	if err != nil {
		return results
	}
	
	// åˆ†å—è§£å‹ï¼ˆæŒ‰===CHUNK===åˆ†éš”ï¼‰
	chunks := bytes.Split(data, []byte("===CHUNK_"))
	
	for _, chunk := range chunks {
		if len(chunk) == 0 {
			continue
		}
		
		// è·³è¿‡æ—¶é—´æˆ³è¡Œ
		idx := bytes.Index(chunk, []byte("\n"))
		if idx == -1 {
			continue
		}
		chunk = chunk[idx+1:]
		
		// è§£å‹
		reader := lz4.NewReader(bytes.NewReader(chunk))
		decompressed, err := io.ReadAll(reader)
		if err != nil {
			continue
		}
		
		// è§£ææ—¥å¿—è¡Œ
		lines := strings.Split(string(decompressed), "\n")
		for i := len(lines) - 1; i >= 0 && len(results) < limit; i-- {
			line := lines[i]
			if line == "" {
				continue
			}
			
			// è§£ææ—¥å¿—
			log := parseLogLine(line)
			
			// å¤šç»´åº¦ç­›é€‰
			if s.matchLogWithFilters(log, keyword, server, level) {
				results = append(results, log)
			}
		}
		
		if len(results) >= limit {
			break
		}
	}
	
	return results
}

func parseLogLine(line string) LogEntry {
	// ç®€å•è§£æ [æ—¶é—´] [çº§åˆ«] [æœåŠ¡å™¨] æ¶ˆæ¯
	// ç”Ÿäº§ç¯å¢ƒåº”è¯¥æ›´å¥å£®
	return LogEntry{
		Timestamp: extractBracket(line, 0),
		Level:     extractBracket(line, 1),
		Server:    extractBracket(line, 2),
		Message:   line,
	}
}

func extractBracket(s string, index int) string {
	count := 0
	start := -1
	for i, c := range s {
		if c == '[' {
			if count == index {
				start = i + 1
			}
			count++
		} else if c == ']' && start != -1 {
			return s[start:i]
		}
	}
	return ""
}

// è·å–ç»Ÿè®¡ä¿¡æ¯
func (s *LogStorage) GetStats() map[string]interface{} {
	s.bufferMu.RLock()
	defer s.bufferMu.RUnlock()
	
	// ç»Ÿè®¡æ‰€æœ‰ä¸åŒçš„æœåŠ¡å™¨
	servers := make(map[string]bool)
	for _, log := range s.memoryBuffer {
		if log.Server != "" {
			servers[log.Server] = true
		}
	}
	
	serverList := make([]string, 0, len(servers))
	for server := range servers {
		serverList = append(serverList, server)
	}
	
	return map[string]interface{}{
		"total_received":    s.stats.TotalReceived,
		"total_compressed":  s.stats.TotalCompressed,
		"in_memory":         len(s.memoryBuffer),
		"compression_ratio": fmt.Sprintf("%.1f:1", s.stats.CompressionRatio),
		"servers":           serverList,
	}
}

func main() {
	storage := NewLogStorage("data")
	metricsStorage := NewMetricsStorage("data", 120) // æ¯å°æœåŠ¡å™¨ä¿ç•™120ä¸ªæ•°æ®ç‚¹ï¼ˆ1å°æ—¶ï¼‰
	
	// API: æ¥æ”¶æ—¥å¿—ï¼ˆå®æ—¶å†™å…¥å†…å­˜ï¼‰
	http.HandleFunc("/api/logs", func(w http.ResponseWriter, r *http.Request) {
		if r.Method != "POST" {
			http.Error(w, "åªæ¥å—POST", http.StatusMethodNotAllowed)
			return
		}
		
		body, _ := io.ReadAll(r.Body)
		var log LogEntry
		
		if err := json.Unmarshal(body, &log); err != nil {
			log = LogEntry{
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
				Message:   string(body),
			}
		}
		
		if log.Timestamp == "" {
			log.Timestamp = time.Now().Format("2006-01-02 15:04:05")
		}
		
		// å®æ—¶è¿½åŠ æ—¥å¿—åˆ°å†…å­˜
		storage.Append(log)
		
		// å¦‚æœåŒ…å«ç›‘æ§æŒ‡æ ‡ï¼Œå­˜å‚¨åˆ° metricsStorage
		// ä»»ä½•å¸¦ server çš„æ—¥å¿—éƒ½ä¼šæ›´æ–°æœåŠ¡å™¨çŠ¶æ€ï¼ˆåŸºäºæœ€åæ¨é€æ—¶é—´ï¼‰
		if log.Metrics != nil && log.Server != "" {
			metricsEntry := MetricsEntry{
				Timestamp: log.Timestamp,
				Server:    log.Server,
				Metrics:   *log.Metrics,
			}
			metricsStorage.Append(metricsEntry)
		}
		
		fmt.Fprintf(w, "âœ“ Received")
	})
	
	// API: æŸ¥è¯¢æ—¥å¿—ï¼ˆå†…å­˜+ç£ç›˜ï¼Œæ”¯æŒå¤šç»´åº¦ç­›é€‰ï¼‰
	http.HandleFunc("/api/query", func(w http.ResponseWriter, r *http.Request) {
		keyword := r.URL.Query().Get("keyword")
		server := r.URL.Query().Get("server")
		level := r.URL.Query().Get("level")
		
		// æŸ¥è¯¢æœ€æ–°çš„1000æ¡ï¼ˆå†…å­˜+ç£ç›˜ï¼‰
		results := storage.Query(keyword, server, level, 1000)
		
		w.Header().Set("Content-Type", "text/plain; charset=utf-8")
		
		for i := len(results) - 1; i >= 0; i-- {
			log := results[i]
			fmt.Fprintf(w, "[%s] [%s] [%s] %s\n",
				log.Timestamp, log.Level, log.Server, log.Message)
		}
	})
	
	// API: ç»Ÿè®¡ä¿¡æ¯
	http.HandleFunc("/api/stats", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		
		// åˆå¹¶æ—¥å¿—å’Œç›‘æ§ç»Ÿè®¡
		logStats := storage.GetStats()
		metricsStats := metricsStorage.GetStats()
		
		combined := make(map[string]interface{})
		for k, v := range logStats {
			combined[k] = v
		}
		for k, v := range metricsStats {
			combined[k] = v
		}
		
		json.NewEncoder(w).Encode(combined)
	})
	
	// API: æŸ¥è¯¢ç›‘æ§æ•°æ®
	http.HandleFunc("/api/metrics", func(w http.ResponseWriter, r *http.Request) {
		server := r.URL.Query().Get("server")
		metricName := r.URL.Query().Get("metric")
		
		// é»˜è®¤è¿”å›æœ€è¿‘ 120 ä¸ªç‚¹ï¼ˆ1å°æ—¶ï¼‰
		limit := 120
		
		w.Header().Set("Content-Type", "application/json")
		
		if server == "" {
			// è¿”å›æ‰€æœ‰æœåŠ¡å™¨çš„æœ€æ–°æ•°æ®
			results := metricsStorage.Query("", metricName, 1)
			json.NewEncoder(w).Encode(results)
		} else {
			// è¿”å›æŒ‡å®šæœåŠ¡å™¨çš„æ—¶åºæ•°æ®
			results := metricsStorage.Query(server, metricName, limit)
			json.NewEncoder(w).Encode(results)
		}
	})
	
	// API: æœåŠ¡å™¨çŠ¶æ€
	http.HandleFunc("/api/servers", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		servers := metricsStorage.GetServerStatus()
		json.NewEncoder(w).Encode(servers)
	})
	
	// API: æœåŠ¡å™¨èšåˆç»Ÿè®¡
	http.HandleFunc("/api/metrics/summary", func(w http.ResponseWriter, r *http.Request) {
		server := r.URL.Query().Get("server")
		
		w.Header().Set("Content-Type", "application/json")
		
		if server == "" {
			// è¿”å›æ‰€æœ‰æœåŠ¡å™¨çš„æ‘˜è¦
			servers := metricsStorage.GetServerStatus()
			summaries := make([]map[string]interface{}, 0)
			for _, s := range servers {
				if summary := metricsStorage.GetAggregatedStats(s.Server); summary != nil {
					summaries = append(summaries, summary)
				}
			}
			json.NewEncoder(w).Encode(summaries)
		} else {
			// è¿”å›æŒ‡å®šæœåŠ¡å™¨çš„æ‘˜è¦
			summary := metricsStorage.GetAggregatedStats(server)
			json.NewEncoder(w).Encode(summary)
		}
	})
	
	// é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆå‰ç«¯é¡µé¢ï¼‰
	http.Handle("/", http.FileServer(http.Dir("static")))
	
	fmt.Println("ğŸš€ MiniLog Lightweight Monitoring Version Started!")
	fmt.Println("ğŸ“Š Web UI: http://localhost:8080")
	fmt.Println("ğŸ“¡ Receive Logs: POST http://localhost:8080/api/logs")
	fmt.Println("ğŸ“ˆ Lightweight Metrics: CPU, Memory, Disk, Load (~50 bytes per push)")
	fmt.Println("ğŸ’¾ Smart Compression: Triggers at 1000 logs or 1 minute")
	fmt.Println("ğŸ” Query Strategy: Memory first â†’ Disk fallback")
	fmt.Println("ğŸ“‰ Monitoring: No heartbeat, status based on log push time")
	http.ListenAndServe(":8080", nil)
}